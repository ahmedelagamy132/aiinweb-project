{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00bcfa1e",
   "metadata": {},
   "source": [
    "\n",
    "## Updated full-stack template walkthrough\n",
    "\n",
    "This lab now uses a Postgres-backed stack managed through Alembic migrations and Docker Compose.\n",
    "\n",
    "- **Database + Alembic**: `backend/alembic/env.py` and `backend/alembic/versions/20250212_initial.py` create tables for echo retries, planner runs, course resources, and RAG document chunks. The new `20250529_agent_runs.py` migration adds the `agent_runs` table for persisting agent executions. Run migrations with `alembic upgrade head` (the backend container executes this automatically on start).\n",
    "- **FastAPI wiring**: `backend/app/database.py` exposes a SQLAlchemy `SessionLocal` dependency. Routers like `app/routers/echo.py`, `app/routers/planner.py`, `app/routers/resources.py`, and `app/routers/agent.py` read and write real rows instead of in-memory mocks.\n",
    "- **Docker Compose + Nginx**: `docker-compose.yml` now launches Postgres, FastAPI (with migrations), the Vite dev server, and an Nginx reverse proxy (`nginx/default.conf`) that fronts both the API (`/api`) and frontend assets.\n",
    "- **RAG chatbot + Agent**: `app/services/rag.py` indexes seeded `DocumentChunk` rows, while `app/services/chatbot.py` blends retrieval with Gemini when `GEMINI_API_KEY` is configured. The release readiness agent also uses RAG to retrieve relevant documentation.\n",
    "- **AI-powered Agent**: The release readiness agent now integrates Gemini for generating strategic insights and AI-powered recommendations, FAISS for RAG retrieval, and persists all runs to the database for auditing.\n",
    "\n",
    "### How to run the stack with Docker Compose\n",
    "\n",
    "1. `cd ai-web`\n",
    "2. `docker compose up --build`\n",
    "3. Open http://localhost:8080 to reach Nginx. API requests are proxied to FastAPI at `/api`. Postgres data lives in the `db_data` volume.\n",
    "\n",
    "### Creating and applying new migrations\n",
    "\n",
    "1. Enter the backend container: `docker compose exec backend bash`\n",
    "2. Generate a migration: `alembic revision -m \"describe change\" --autogenerate`\n",
    "3. Apply migrations: `alembic upgrade head`\n",
    "\n",
    "### Testing the new features\n",
    "\n",
    "- **Echo retry + persistence**: Submit the echo form; the \"Recent echo attempts\" list should update from the `echo_attempts` table.\n",
    "- **Resource Board**: Add a URL in the Resource Board. Refreshing the page keeps entries thanks to the `resources` table.\n",
    "- **Planner + history**: Generate a plan in the Planner panel; the newest plan appears in the history list powered by the `plan_runs` table.\n",
    "- **Release Readiness Agent**: Select a feature, run the agent, and see AI-powered recommendations with Gemini insights, RAG-retrieved context, and historical runs from the `agent_runs` table.\n",
    "\n",
    "Refer to the updated source files when walking through the lab so students can trace how migrations, database sessions, and the React UI connect end to end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a966c8",
   "metadata": {},
   "source": [
    "\n",
    "## Why Nginx, Alembic, and FAISS matter for this stack\n",
    "\n",
    "**Nginx reverse proxy**\n",
    "- *What it is*: A lightweight web server that terminates HTTP, proxies requests, and serves static assets.\n",
    "- *How we integrated it*: `docker-compose.yml` launches an `nginx` service with `nginx/default.conf` routing `/api/` to FastAPI (port 8000) and everything else to the Vite dev server (port 5173). This keeps the release readiness endpoint (`/api/ai/release-readiness`) and the React UI on a single origin behind http://localhost:8080.\n",
    "- *Why we adopted it*: A unified front door eliminates CORS headaches and mirrors production topologies where a reverse proxy fronts both the SPA and API.\n",
    "- *If we removed it*: Students would juggle multiple ports, browser `fetch` calls would hit CORS errors, and any deployed version would need a different networking story than the local lab.\n",
    "\n",
    "**Alembic migrations**\n",
    "- *What it is*: A schema migration tool for SQLAlchemy projects so database changes are reproducible and versioned.\n",
    "- *How we integrated it*: `backend/alembic/env.py` reads `DATABASE_URL`, and migrations create tables for echo retries, plan runs, resources, RAG document chunks, and now agent runs. The backend container runs `python -m alembic upgrade head` before starting Uvicorn, so every boot applies the latest schema and seeds data.\n",
    "- *Why we adopted it*: Planner history, the resources board, RAG context for the chatbot, and agent execution history all live in Postgres. Alembic keeps the schema in sync across teammates so future agent runs can safely persist outputs or read existing context without surprises.\n",
    "- *If we removed it*: Each developer would run ad hoc SQL by hand, migrations would get lost, and any endpoint touching `plan_runs`, `resources`, `document_chunks`, or `agent_runs` would fail at runtime.\n",
    "\n",
    "**FAISS retrieval**\n",
    "- *What it is*: A fast similarity search library for vector embeddings, used here for lightweight RAG.\n",
    "- *How we integrated it*: `app/services/rag.py` normalizes document embeddings and loads them into a FAISS `IndexFlatL2`; both the chatbot service and the release readiness agent call `build_retriever()` to fetch the top context chunks before generating an answer or recommendations.\n",
    "- *Why we adopted it*: It gives agents a deterministic context window without external APIs, so answers stay grounded in the course docs even offline.\n",
    "- *If we removed it*: The chatbot and agent would fall back to generic responses with no retrieved evidence, reducing accuracy and making the lab less realistic.\n",
    "\n",
    "### Agent foundation at a glance\n",
    "- **Tools**: `app/services/agent_tools.py` defines deterministic product briefs, launch windows, support contacts, and SLO watch items so the agent has stable inputs.\n",
    "- **RAG Integration**: The agent retrieves relevant documentation using FAISS before generating recommendations.\n",
    "- **Gemini AI**: When `GEMINI_API_KEY` is configured, the agent uses Gemini to generate strategic insights and AI-powered recommendations.\n",
    "- **Database Persistence**: All agent runs are saved to the `agent_runs` table for auditing and learning.\n",
    "- **Orchestration**: `run_release_readiness_agent()` in `app/services/agent.py` sequences tool calls, retrieves RAG context, calls Gemini, builds a plan, persists the run, and returns structured recommendations with tool traces.\n",
    "- **API surface**: `app/routers/agent.py` exposes `/ai/release-readiness`, `/ai/history`, and `/ai/features`, keeping the router thin and delegating all logic to the service layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faiss_embeddings_deep_dive",
   "metadata": {},
   "source": [
    "## Deep Dive: FAISS, Embeddings, and RAG Integration\n",
    "\n",
    "### Understanding Embeddings\n",
    "\n",
    "**What are embeddings?**\n",
    "Embeddings are numerical representations of text (or other data) that capture semantic meaning. Think of them as coordinates in a high-dimensional space where semantically similar texts are positioned close together.\n",
    "\n",
    "**Why do we need embeddings?**\n",
    "- **Semantic search**: Find documents by meaning, not just keyword matching\n",
    "- **Similarity comparison**: Determine how similar two pieces of text are\n",
    "- **Efficient retrieval**: Search through millions of documents quickly\n",
    "\n",
    "**How embeddings work:**\n",
    "1. **Text → Vector**: Convert text into a fixed-size numerical vector (e.g., 256 or 768 dimensions)\n",
    "2. **Semantic proximity**: Similar meanings produce similar vectors\n",
    "3. **Distance metrics**: Use cosine similarity or L2 distance to find similar vectors\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Simplified conceptual example\n",
    "embed(\"FastAPI tutorial\") → [0.2, 0.8, ..., 0.5]  # 256 numbers\n",
    "embed(\"FastAPI guide\")    → [0.3, 0.7, ..., 0.4]  # Very similar!\n",
    "embed(\"Pizza recipe\")     → [0.9, 0.1, ..., 0.2]  # Very different\n",
    "```\n",
    "\n",
    "### What is FAISS?\n",
    "\n",
    "**FAISS (Facebook AI Similarity Search)** is a library developed by Meta for efficient similarity search and clustering of dense vectors.\n",
    "\n",
    "**Key features:**\n",
    "- **Speed**: Search billions of vectors in milliseconds\n",
    "- **Scalability**: Works on CPU and GPU\n",
    "- **Flexibility**: Multiple index types for different use cases\n",
    "- **Memory efficiency**: Optimized data structures\n",
    "\n",
    "**Common FAISS index types:**\n",
    "1. **IndexFlatL2**: Exact search using L2 (Euclidean) distance\n",
    "   - Most accurate, good for small datasets\n",
    "   - Our implementation uses this for simplicity\n",
    "2. **IndexFlatIP**: Exact search using inner product (cosine similarity)\n",
    "   - Good when vectors are normalized\n",
    "3. **IndexIVFFlat**: Approximate search with inverted file index\n",
    "   - Faster for large datasets, slight accuracy tradeoff\n",
    "\n",
    "### How RAG (Retrieval-Augmented Generation) Works\n",
    "\n",
    "**RAG combines retrieval and generation:**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────┐\n",
    "│  User Query: \"How do I deploy the agent?\"           │\n",
    "└───────────────────┬─────────────────────────────────┘\n",
    "                    ▼\n",
    "┌─────────────────────────────────────────────────────┐\n",
    "│  Step 1: Embed Query                                │\n",
    "│  query_vector = embed(\"How do I deploy the agent?\") │\n",
    "└───────────────────┬─────────────────────────────────┘\n",
    "                    ▼\n",
    "┌─────────────────────────────────────────────────────┐\n",
    "│  Step 2: FAISS Search                               │\n",
    "│  Find top 3 most similar document chunks            │\n",
    "│  Results: [doc1 (score: 0.23), doc2 (0.45), ...]   │\n",
    "└───────────────────┬─────────────────────────────────┘\n",
    "                    ▼\n",
    "┌─────────────────────────────────────────────────────┐\n",
    "│  Step 3: Build Context                              │\n",
    "│  Combine retrieved chunks into a context string     │\n",
    "└───────────────────┬─────────────────────────────────┘\n",
    "                    ▼\n",
    "┌─────────────────────────────────────────────────────┐\n",
    "│  Step 4: Generate with AI                           │\n",
    "│  Pass context + query to Gemini for answer         │\n",
    "│  Result: Grounded, accurate response                │\n",
    "└─────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Why RAG is powerful:**\n",
    "- **Grounding**: AI responses are backed by actual documents\n",
    "- **Up-to-date**: Add new documents without retraining\n",
    "- **Transparency**: Show which documents were used\n",
    "- **Reduced hallucination**: AI has factual context to work with\n",
    "\n",
    "### Our RAG Implementation in `app/services/rag.py`\n",
    "\n",
    "Let's break down how our implementation works:\n",
    "\n",
    "#### 1. Creating Embeddings (Deterministic)\n",
    "\n",
    "```python\n",
    "def embed_text(text: str) -> np.ndarray:\n",
    "    \"\"\"Create a deterministic hashed embedding without external dependencies.\"\"\"\n",
    "    vector = np.zeros(EMBED_DIM, dtype=\"float32\")  # 256 dimensions\n",
    "    for token in _tokenize(text):  # Split text into words\n",
    "        vector[hash(token) % EMBED_DIM] += 1.0  # Hash each word to a dimension\n",
    "    norm = np.linalg.norm(vector)\n",
    "    if norm:\n",
    "        vector /= norm  # Normalize to unit length\n",
    "    return vector\n",
    "```\n",
    "\n",
    "**What's happening:**\n",
    "- Uses a **bag-of-words** approach with hashing\n",
    "- Each word maps to a dimension via hash function\n",
    "- Normalizes the vector so all vectors have length 1\n",
    "- **Why deterministic?** Same text always produces same embedding (no API calls, offline-friendly)\n",
    "- **Production alternative:** Use real embedding models like `text-embedding-004` from Gemini or OpenAI's embeddings\n",
    "\n",
    "#### 2. Building the FAISS Index\n",
    "\n",
    "```python\n",
    "class Retriever:\n",
    "    def __init__(self, chunks: Sequence[DocumentChunk]):\n",
    "        self.chunks = list(chunks)\n",
    "        if not self.chunks:\n",
    "            self.index = None\n",
    "            return\n",
    "\n",
    "        # Create FAISS index for L2 (Euclidean) distance\n",
    "        self.index = faiss.IndexFlatL2(EMBED_DIM)  # 256 dimensions\n",
    "        \n",
    "        # Stack all embeddings into a numpy array\n",
    "        embeddings = np.stack([\n",
    "            np.array(chunk.embedding, dtype=\"float32\") \n",
    "            for chunk in self.chunks\n",
    "        ])\n",
    "        \n",
    "        # Add to index (builds internal data structures)\n",
    "        self.index.add(embeddings)\n",
    "```\n",
    "\n",
    "**What's happening:**\n",
    "- Creates a FAISS index that uses **L2 distance** (Euclidean distance)\n",
    "- Loads all pre-computed embeddings from database\n",
    "- Adds them to FAISS index for fast searching\n",
    "\n",
    "#### 3. Searching for Relevant Context\n",
    "\n",
    "```python\n",
    "def search(self, query: str, k: int = 3) -> list[RetrievedContext]:\n",
    "    if not self.index or not self.chunks:\n",
    "        return []\n",
    "\n",
    "    # Embed the query\n",
    "    query_vector = np.expand_dims(embed_text(query), axis=0)\n",
    "    \n",
    "    # Search FAISS index for k nearest neighbors\n",
    "    distances, indices = self.index.search(query_vector, min(k, len(self.chunks)))\n",
    "    \n",
    "    # Build results with content, source, and score\n",
    "    results: list[RetrievedContext] = []\n",
    "    for score, idx in zip(distances[0], indices[0]):\n",
    "        if idx == -1:\n",
    "            continue\n",
    "        chunk = self.chunks[idx]\n",
    "        results.append(\n",
    "            RetrievedContext(\n",
    "                content=chunk.content,\n",
    "                source=chunk.source,\n",
    "                score=float(score)  # Lower is better for L2 distance\n",
    "            )\n",
    "        )\n",
    "    return results\n",
    "```\n",
    "\n",
    "**What's happening:**\n",
    "1. **Embed the query** using the same function as documents\n",
    "2. **Search FAISS** for the k closest vectors\n",
    "3. **Return results** with the actual content, source, and similarity score\n",
    "4. **Lower score = more similar** (L2 distance)\n",
    "\n",
    "#### 4. Integration in the Agent\n",
    "\n",
    "The agent uses RAG in `app/services/agent.py`:\n",
    "\n",
    "```python\n",
    "# Build retriever from database chunks\n",
    "retriever = build_retriever(db)\n",
    "\n",
    "# Create search query combining feature name and context\n",
    "search_query = f\"{brief.name} {context.audience_role} release launch\"\n",
    "\n",
    "# Retrieve top 3 relevant chunks\n",
    "rag_contexts = retriever.search(search_query, k=3)\n",
    "\n",
    "# Pass to Gemini for AI-powered insights\n",
    "gemini_insight, ai_recommendations = _generate_gemini_insight(\n",
    "    brief, launch_window, slo_items, rag_contexts, context\n",
    ")\n",
    "```\n",
    "\n",
    "**The flow:**\n",
    "1. **Load** all document chunks from PostgreSQL\n",
    "2. **Build** FAISS index from embeddings\n",
    "3. **Search** for relevant context based on feature + audience\n",
    "4. **Pass** retrieved context to Gemini along with other data\n",
    "5. **Generate** AI-powered recommendations grounded in documentation\n",
    "\n",
    "### Why This Matters for Production\n",
    "\n",
    "**Without RAG:**\n",
    "```\n",
    "User: \"How do I deploy the agent?\"\n",
    "AI: \"You can deploy using Docker or Kubernetes...\"  # Generic answer, might be wrong\n",
    "```\n",
    "\n",
    "**With RAG:**\n",
    "```\n",
    "User: \"How do I deploy the agent?\"\n",
    "System: [Retrieves actual deployment docs]\n",
    "AI: \"According to the deployment guide, run `docker compose up --build` \n",
    "     in the ai-web directory. The agent will be available at localhost:8080.\"\n",
    "     # Accurate, grounded in actual docs\n",
    "```\n",
    "\n",
    "**Key benefits:**\n",
    "- **Accuracy**: Answers come from verified documentation\n",
    "- **Auditability**: Can show which docs were used\n",
    "- **Maintainability**: Update docs without retraining AI\n",
    "- **Cost-effective**: No need to fine-tune models\n",
    "\n",
    "### Performance Considerations\n",
    "\n",
    "**Current implementation:**\n",
    "- **Index type**: IndexFlatL2 (exact search)\n",
    "- **Embedding method**: Deterministic hashing\n",
    "- **Dataset size**: Dozens of chunks (course docs)\n",
    "- **Speed**: Milliseconds per search\n",
    "\n",
    "**For production scale:**\n",
    "- **Larger datasets** (millions of chunks): Use `IndexIVFFlat` or `IndexHNSW`\n",
    "- **Better embeddings**: Switch to `text-embedding-004` from Gemini\n",
    "- **Caching**: Cache frequent queries to avoid repeated searches\n",
    "- **Reranking**: Use a second model to rerank retrieved results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c2cc28",
   "metadata": {},
   "source": [
    "# Lab 05 · AI-Powered Release Readiness Agent\n",
    "\n",
    "*This lab notebook provides guided steps. All commands are intended for local execution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15651818",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Build a production-grade release readiness agent that integrates with Gemini AI, FAISS RAG, and database persistence.\n",
    "- Provide credible tool abstractions for product briefs, launch windows, and stakeholder contacts.\n",
    "- Expose `/ai/release-readiness`, `/ai/history`, and `/ai/features` endpoints that return structured recommendations.\n",
    "- Persist agent runs to the database for auditing and learning from past executions.\n",
    "\n",
    "In this lab, you will build a **production-grade AI-powered agent** that demonstrates:\n",
    "\n",
    "1. **Model an agent workflow**: Learn how agents orchestrate multiple tools, RAG retrieval, and AI calls to accomplish complex tasks\n",
    "2. **Build tool abstractions**: Create reusable functions that agents can call to gather information\n",
    "3. **Integrate AI capabilities**: Use Gemini to generate intelligent insights and recommendations\n",
    "4. **Implement RAG**: Retrieve relevant documentation using FAISS to ground agent responses\n",
    "5. **Persist agent runs**: Store execution history in PostgreSQL for auditing and learning\n",
    "6. **Structure agent responses**: Return well-formatted JSON with AI insights, recommendations, and tool traces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da98749",
   "metadata": {},
   "source": [
    "## What will be learned\n",
    "- Integrating Gemini AI for generating strategic insights and recommendations.\n",
    "- Using FAISS-backed RAG to retrieve relevant documentation.\n",
    "- Persisting agent runs to PostgreSQL for auditing and compliance.\n",
    "- Coordinating multiple service helpers inside an agent workflow.\n",
    "- Returning Pydantic models that the frontend can render without extra parsing.\n",
    "- Logging tool invocations to aid observability and debugging.\n",
    "\n",
    "By the end of this lab, you will understand:\n",
    "\n",
    "### Core Concepts\n",
    "- **Agent Architecture**: How agents break down complex tasks into smaller tool calls, RAG retrieval, and AI generation\n",
    "- **Tool Design**: Creating focused functions that do one thing well\n",
    "- **RAG Integration**: Retrieving relevant context to ground AI responses\n",
    "- **AI Integration**: Using Gemini to generate intelligent insights\n",
    "- **Database Persistence**: Storing agent runs for auditing and learning\n",
    "- **Service Layer Pattern**: Separating business logic from API routing\n",
    "\n",
    "### Technical Skills\n",
    "- **Coordinating service helpers**: Calling multiple functions within an agent workflow\n",
    "- **Pydantic models**: Defining schemas that validate data and generate documentation\n",
    "- **Error handling**: Gracefully managing missing data or invalid inputs\n",
    "- **Logging tool invocations**: Tracking which tools were called and what they returned\n",
    "- **Priority-based recommendations**: Categorizing actions by urgency (high/medium/low)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b18c2",
   "metadata": {},
   "source": [
    "## Prerequisites & install\n",
    "Reuse the virtual environment created earlier in the course. The required dependencies are already in `requirements.txt`.\n",
    "\n",
    "```bash\n",
    "cd ai-web/backend\n",
    ". .venv/bin/activate\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Environment Setup\n",
    "Reuse the virtual environment created earlier in the course. This lab builds on the FastAPI foundation from previous labs.\n",
    "\n",
    "### Required Dependencies\n",
    "The following are needed (already in requirements.txt):\n",
    "- `pydantic` - Data validation and type hints\n",
    "- `google-generativeai` - Gemini API client\n",
    "- `faiss-cpu` - FAISS vector similarity search\n",
    "- `sqlalchemy` - Database ORM\n",
    "- `alembic` - Database migrations\n",
    "\n",
    "### Gemini API Key (Optional but Recommended)\n",
    "To enable AI-powered insights, add your Gemini API key to `backend/.env`:\n",
    "```\n",
    "GEMINI_API_KEY=your-api-key-here\n",
    "```\n",
    "The agent works without it but will only return deterministic recommendations.\n",
    "\n",
    "### Verification\n",
    "Ensure your backend is running and accessible:\n",
    "```bash\n",
    "curl http://localhost:8000/health\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ada4db",
   "metadata": {},
   "source": [
    "## Step-by-step tasks\n",
    "Build out the tools, service, and router layers needed to run the AI-powered release readiness agent.\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "Before diving into code, let's understand the enhanced architecture:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────┐\n",
    "│  FastAPI Router (agent.py)             │  ← HTTP endpoints\n",
    "│  - /ai/release-readiness               │\n",
    "│  - /ai/history                          │\n",
    "│  - /ai/features                         │\n",
    "└─────────────────┬───────────────────────┘\n",
    "                  │\n",
    "                  ▼\n",
    "┌─────────────────────────────────────────┐\n",
    "│  Agent Service (agent.py)               │  ← Business logic\n",
    "│  - Orchestrates tool calls              │\n",
    "│  - Retrieves RAG context (FAISS)        │\n",
    "│  - Calls Gemini for AI insights         │\n",
    "│  - Persists runs to database            │\n",
    "│  - Builds recommendations               │\n",
    "└─────────────────┬───────────────────────┘\n",
    "                  │\n",
    "        ┌─────────┼─────────┐\n",
    "        ▼         ▼         ▼\n",
    "┌───────────┐ ┌───────┐ ┌─────────┐\n",
    "│  Tools    │ │ RAG   │ │ Gemini  │\n",
    "│  (agent_  │ │(rag.py)│ │  API   │\n",
    "│  tools.py)│ │       │ │        │\n",
    "└───────────┘ └───────┘ └─────────┘\n",
    "```\n",
    "\n",
    "This separation of concerns makes the system:\n",
    "- **Testable**: Each layer can be tested independently\n",
    "- **Maintainable**: Changes to one layer don't ripple through the entire system\n",
    "- **Reusable**: Tools can be used by multiple agents\n",
    "- **Auditable**: All runs are persisted for review\n",
    "\n",
    "Let's build each layer from the bottom up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa103e09",
   "metadata": {},
   "source": [
    "### Step 1: Release data tools\n",
    "The tools in `app/services/agent_tools.py` provide deterministic product data so the agent can make realistic decisions.\n",
    "\n",
    "#### What Are Tools?\n",
    "Tools are **focused functions** that agents call to gather information or perform actions. Each tool should:\n",
    "- Do **one thing well**\n",
    "- Have **clear inputs and outputs**\n",
    "- Be **stateless** (no hidden dependencies)\n",
    "- Return **structured data** (Pydantic models, not raw dicts)\n",
    "\n",
    "#### The Four Core Tools\n",
    "\n",
    "1. **`fetch_feature_brief(feature_slug)`**: Returns product information\n",
    "2. **`fetch_launch_window(feature_slug)`**: Returns deployment timing\n",
    "3. **`fetch_support_contacts(audience_role)`**: Returns stakeholders to notify\n",
    "4. **`list_slo_watch_items(feature_slug)`**: Returns reliability concerns\n",
    "\n",
    "Review the existing implementation in `app/services/agent_tools.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54450c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the existing tools\n",
    "from app.services.agent_tools import (\n",
    "    fetch_feature_brief,\n",
    "    fetch_launch_window,\n",
    "    fetch_support_contacts,\n",
    "    list_slo_watch_items,\n",
    ")\n",
    "\n",
    "# Test each tool\n",
    "print(\"Feature Brief:\")\n",
    "print(fetch_feature_brief(\"curriculum-pathways\").model_dump())\n",
    "print(\"\\nLaunch Window:\")\n",
    "print(fetch_launch_window(\"curriculum-pathways\").model_dump())\n",
    "print(\"\\nSupport Contacts:\")\n",
    "print([c.model_dump() for c in fetch_support_contacts(\"Instructor\")])\n",
    "print(\"\\nSLO Watch Items:\")\n",
    "print(list_slo_watch_items(\"curriculum-pathways\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a708bd7b",
   "metadata": {},
   "source": [
    "### Step 2: AI-Powered Agent Service\n",
    "The agent service in `app/services/agent.py` orchestrates tools, RAG, Gemini, and database persistence.\n",
    "\n",
    "#### Key Components\n",
    "\n",
    "**1. Tool Orchestration**\n",
    "The agent calls tools to gather context:\n",
    "```python\n",
    "brief = fetch_feature_brief(context.feature_slug)\n",
    "launch_window = fetch_launch_window(context.feature_slug)\n",
    "contacts = fetch_support_contacts(context.audience_role)\n",
    "slo_watch_items = list_slo_watch_items(context.feature_slug)\n",
    "```\n",
    "\n",
    "**2. RAG Retrieval (FAISS)**\n",
    "The agent retrieves relevant documentation:\n",
    "```python\n",
    "retriever = build_retriever(db)\n",
    "rag_contexts = retriever.search(search_query, k=3)\n",
    "```\n",
    "\n",
    "**3. Gemini AI Insights**\n",
    "When configured, Gemini generates strategic insights:\n",
    "```python\n",
    "gemini_insight, ai_recommendations = _generate_gemini_insight(\n",
    "    brief, launch_window, slo_items, rag_contexts, context\n",
    ")\n",
    "```\n",
    "\n",
    "**4. Database Persistence**\n",
    "All runs are saved for auditing:\n",
    "```python\n",
    "agent_run = AgentRun(\n",
    "    feature_slug=context.feature_slug,\n",
    "    summary=summary,\n",
    "    gemini_insight=gemini_insight,\n",
    "    recommended_actions=[...],\n",
    "    tool_calls=[...],\n",
    "    rag_contexts=[...],\n",
    "    used_gemini=used_gemini,\n",
    ")\n",
    "db.add(agent_run)\n",
    "db.commit()\n",
    "```\n",
    "\n",
    "#### Enhanced Output Structure\n",
    "\n",
    "```python\n",
    "class AgentRunResult(BaseModel):\n",
    "    summary: str\n",
    "    gemini_insight: str | None = None  # AI-generated insight\n",
    "    recommended_actions: list[AgentRecommendation]  # With priority levels\n",
    "    plan: Plan\n",
    "    tool_calls: list[AgentToolCall]  # Includes RAG and Gemini calls\n",
    "    rag_contexts: list[RAGContext] = []  # Retrieved documents\n",
    "    used_gemini: bool = False  # Whether Gemini was used\n",
    "```\n",
    "\n",
    "#### Recommendation Priorities\n",
    "\n",
    "Recommendations now have priority levels:\n",
    "```python\n",
    "class AgentRecommendation(BaseModel):\n",
    "    title: str\n",
    "    detail: str\n",
    "    priority: Literal[\"high\", \"medium\", \"low\"] = \"medium\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70444e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the agent service structure\n",
    "from app.services.agent import (\n",
    "    AgentRunContext,\n",
    "    AgentRunResult,\n",
    "    AgentRecommendation,\n",
    "    AgentToolCall,\n",
    "    RAGContext,\n",
    ")\n",
    "\n",
    "# Show the enhanced models\n",
    "print(\"AgentRunResult fields:\")\n",
    "for name, field in AgentRunResult.model_fields.items():\n",
    "    print(f\"  - {name}: {field.annotation}\")\n",
    "\n",
    "print(\"\\nAgentRecommendation fields:\")\n",
    "for name, field in AgentRecommendation.model_fields.items():\n",
    "    print(f\"  - {name}: {field.annotation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e666451f",
   "metadata": {},
   "source": [
    "### Step 3: API Router with Multiple Endpoints\n",
    "The router in `app/routers/agent.py` exposes three endpoints.\n",
    "\n",
    "#### Endpoint 1: `/ai/release-readiness` (POST)\n",
    "Runs the agent and returns structured recommendations:\n",
    "```python\n",
    "@router.post(\"/release-readiness\", response_model=AgentRunResult)\n",
    "def release_readiness(\n",
    "    payload: AgentRunContext,\n",
    "    db: Session = Depends(get_db),\n",
    ") -> AgentRunResult:\n",
    "    return run_release_readiness_agent(payload, db=db)\n",
    "```\n",
    "\n",
    "#### Endpoint 2: `/ai/history` (GET)\n",
    "Returns historical agent runs for auditing:\n",
    "```python\n",
    "@router.get(\"/history\", response_model=AgentHistoryResponse)\n",
    "def agent_history(\n",
    "    feature_slug: str | None = Query(None),\n",
    "    limit: int = Query(10, ge=1, le=50),\n",
    "    db: Session = Depends(get_db),\n",
    ") -> AgentHistoryResponse:\n",
    "    runs = get_agent_history(db, feature_slug=feature_slug, limit=limit)\n",
    "    return AgentHistoryResponse(runs=items, total=len(items))\n",
    "```\n",
    "\n",
    "#### Endpoint 3: `/ai/features` (GET)\n",
    "Lists available features for the agent:\n",
    "```python\n",
    "@router.get(\"/features\")\n",
    "def list_available_features() -> dict[str, Any]:\n",
    "    # Returns feature metadata for frontend dropdowns\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42558454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The router is already configured. Verify the endpoints exist:\n",
    "from app.routers.agent import router\n",
    "\n",
    "print(\"Agent router endpoints:\")\n",
    "for route in router.routes:\n",
    "    if hasattr(route, 'methods'):\n",
    "        print(f\"  {list(route.methods)[0]:6} {route.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c0d1cc",
   "metadata": {},
   "source": [
    "### Step 4: Database Model for Agent Runs\n",
    "The `AgentRun` model in `app/models.py` persists agent executions.\n",
    "\n",
    "```python\n",
    "class AgentRun(Base):\n",
    "    __tablename__ = \"agent_runs\"\n",
    "\n",
    "    id: Mapped[int] = mapped_column(Integer, primary_key=True)\n",
    "    feature_slug: Mapped[str] = mapped_column(String(120), index=True)\n",
    "    audience_role: Mapped[str] = mapped_column(String(120))\n",
    "    audience_experience: Mapped[str] = mapped_column(String(32))\n",
    "    summary: Mapped[str] = mapped_column(Text)\n",
    "    gemini_insight: Mapped[str | None] = mapped_column(Text, nullable=True)\n",
    "    recommended_actions: Mapped[Any] = mapped_column(JSONB)\n",
    "    tool_calls: Mapped[Any] = mapped_column(JSONB)\n",
    "    rag_contexts: Mapped[Any] = mapped_column(JSONB, default=list)\n",
    "    used_gemini: Mapped[bool] = mapped_column(Boolean, default=False)\n",
    "    created_at: Mapped[datetime] = mapped_column(DateTime, default=utcnow)\n",
    "```\n",
    "\n",
    "The migration `alembic/versions/20250529_agent_runs.py` creates this table and seeds feature-related document chunks for RAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45f79c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the AgentRun model\n",
    "from app.models import AgentRun\n",
    "\n",
    "print(\"AgentRun table columns:\")\n",
    "for column in AgentRun.__table__.columns:\n",
    "    print(f\"  - {column.name}: {column.type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26208bf4",
   "metadata": {},
   "source": [
    "### Step 5: Execute the Agent\n",
    "Call the agent to see the full output with AI insights, RAG contexts, and tool traces.\n",
    "\n",
    "#### What to Look For\n",
    "\n",
    "When you call `run_release_readiness_agent()`, inspect:\n",
    "\n",
    "1. **Summary**: Clear and actionable overview\n",
    "2. **Gemini Insight**: AI-generated strategic assessment (when API key configured)\n",
    "3. **Recommended actions**: Priority-coded (high/medium/low)\n",
    "4. **Tool calls**: Including RAG retrieval and Gemini insight generation\n",
    "5. **RAG contexts**: Retrieved documents from FAISS\n",
    "6. **Plan**: Nested planner output\n",
    "\n",
    "#### Expected Output Structure\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"summary\": \"Curriculum Pathways targets Instructor personas...\",\n",
    "  \"gemini_insight\": \"Strategic launch assessment: The feature...\",\n",
    "  \"recommended_actions\": [\n",
    "    {\n",
    "      \"title\": \"Confirm launch communications\",\n",
    "      \"detail\": \"Share the feature brief with...\",\n",
    "      \"priority\": \"high\"\n",
    "    },\n",
    "    {\n",
    "      \"title\": \"[AI] Review documentation coverage\",\n",
    "      \"detail\": \"Gemini recommends...\",\n",
    "      \"priority\": \"medium\"\n",
    "    }\n",
    "  ],\n",
    "  \"tool_calls\": [\n",
    "    {\"tool\": \"fetch_feature_brief\", ...},\n",
    "    {\"tool\": \"rag_retrieval\", ...},\n",
    "    {\"tool\": \"gemini_insight_generation\", ...}\n",
    "  ],\n",
    "  \"rag_contexts\": [\n",
    "    {\"content\": \"...\", \"source\": \"docs/agent\", \"score\": 0.23}\n",
    "  ],\n",
    "  \"used_gemini\": true,\n",
    "  \"plan\": {...}\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2edbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from app.services.agent import AgentRunContext, run_release_readiness_agent\n",
    "from app.database import SessionLocal\n",
    "\n",
    "# Create a database session (in Docker, this connects to PostgreSQL)\n",
    "db = SessionLocal()\n",
    "\n",
    "try:\n",
    "    context = AgentRunContext(\n",
    "        feature_slug='curriculum-pathways',\n",
    "        launch_date=date(2025, 3, 10),\n",
    "        audience_role='Instructor',\n",
    "        audience_experience='intermediate',\n",
    "    )\n",
    "\n",
    "    result = run_release_readiness_agent(context, db=db)\n",
    "    \n",
    "    # Display key results\n",
    "    print(\"=== Summary ===\")\n",
    "    print(result.summary)\n",
    "    \n",
    "    print(\"\\n=== Gemini Insight ===\")\n",
    "    print(result.gemini_insight or \"(Gemini not configured)\")\n",
    "    \n",
    "    print(\"\\n=== Recommendations ===\")\n",
    "    for rec in result.recommended_actions:\n",
    "        print(f\"  [{rec.priority.upper()}] {rec.title}\")\n",
    "    \n",
    "    print(\"\\n=== Tool Calls ===\")\n",
    "    for tc in result.tool_calls:\n",
    "        print(f\"  - {tc.tool}: {tc.output_preview[:50]}...\")\n",
    "    \n",
    "    print(f\"\\n=== Used Gemini: {result.used_gemini} ===\")\n",
    "finally:\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b805f56f",
   "metadata": {},
   "source": [
    "## Validation / acceptance checks\n",
    "\n",
    "### What to Verify\n",
    "\n",
    "After implementing all steps, test that:\n",
    "\n",
    "1. **The endpoint responds**: `curl` succeeds with 200 OK\n",
    "2. **The response is structured**: JSON contains `summary`, `gemini_insight`, `recommended_actions`, `plan`, `tool_calls`, `rag_contexts`, `used_gemini`\n",
    "3. **History endpoint works**: `GET /ai/history` returns past runs\n",
    "4. **Features endpoint works**: `GET /ai/features` lists available features\n",
    "5. **The documentation works**: FastAPI docs (`/docs`) show all endpoints\n",
    "6. **Error handling works**: Invalid input returns helpful error messages\n",
    "\n",
    "### Manual Testing\n",
    "\n",
    "```bash\n",
    "# Run the agent\n",
    "curl -X POST http://localhost:8000/ai/release-readiness \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\"feature_slug\":\"curriculum-pathways\",\"launch_date\":\"2025-03-10\",\"audience_role\":\"Instructor\",\"audience_experience\":\"intermediate\"}'\n",
    "\n",
    "# Get agent history\n",
    "curl http://localhost:8000/ai/history\n",
    "\n",
    "# Get available features\n",
    "curl http://localhost:8000/ai/features\n",
    "```\n",
    "\n",
    "### Expected Success Criteria\n",
    "\n",
    "- ✅ The response includes a summary, recommendations with priorities, and tool call traces\n",
    "- ✅ When Gemini is configured, `gemini_insight` contains AI-generated text and `used_gemini` is true\n",
    "- ✅ The `rag_contexts` array contains retrieved documents with scores\n",
    "- ✅ The history endpoint returns previously executed agent runs\n",
    "- ✅ The FastAPI interactive docs (`/docs`) display all three endpoints under the **ai** tag\n",
    "- ✅ React development mode renders the structured agent output with AI insights and RAG contexts\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
